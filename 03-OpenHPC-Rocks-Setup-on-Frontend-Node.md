# OpenHPC-Rocks Setup on Frontend Node

This recipe outlines the OpenHPC-Rocks setup on the frontend-node. It is based on the [Rocky Linux 9 Install Guide (with Warewulf4 + Slurm)](https://github.com/openhpc/ohpc/releases/download/v3.2.GA/Install_guide-Rocky9-Warewulf4-SLURM-3.2-x86_64.pdf), but tries to be much more user-friendly.

## Warewulf Metapackage Install

Install OpenHPC repositories, GPG key rings and basic Warewulf packages.

```bash
dnf -y install http://repos.openhpc.community/OpenHPC/3/EL_9/x86_64/ohpc-release-3-1.el9.x86_64.rpm
dnf -y install dnf-plugins-core
dnf config-manager --set-enabled crb
dnf -y install ohpc-base warewulf-ohpc hwloc-ohpc nhc-ohpc munge
```

## MUNGE Install and Key Creation

Check munge is installed and set up:

```bash
# Create key
/usr/sbin/create-munge-key
# Enable and start service
systemctl enable --now munge.service
```

## Time Server Setup

Edit `/etc/chrony.conf`:

```bash
...
# Allow NTP client access from private cluster network.
allow 172.16.79.0/24

# Serve time even if not synchronized to a time source.
local stratum 10
...
```

The `allow` directive matches to the local network spanned via the second network interface `ens192`. Restart the service with:

```bash
systemctl enable --now chronyd
```

If desired, change to 24h time display format on the frontend-node with:

```bash
localectl set-locale LC_TIME=C.UTF8
```

You may wish to check `chronyd` logs in `/var/log/messages`.

## Enable essential systemd-Services

Persistent services are:

```bash
# do not yet enable with --now
systemctl enable dhcpd			
systemctl enable tftp
systemctl enable nfs-server
```

## Modify Overlay Templates for Provisioning

There are three predefined image overlays:

- `wwinit`: System overlay for compute-nodes
- `generic`: Runtime overlay for compute-nodes
- `host`: Overlay for frontend-node

All overlay files contain text items within double brackets `{{ ... }}`, which are subject to Warewulf's `text/template` engine. See https://warewulf.org/docs/main/contents/templating.html and https://pkg.go.dev/text/template for more information on this topic.

### Overlay `wwinit`

Edit `/srv/warewulf/overlays/wwinit/rootfs/etc/hostname.ww`:

```bash
# This block is autogenerated by warewulf
# Host:   {{.BuildHost}}
# Time:   {{.BuildTime}}
# Source: {{.BuildSource}}
{{$.Id}}
```

### Overlay `generic`

Replace `/srv/warewulf/overlays/generic/rootfs/etc/hosts.ww` with the following content.

> [!IMPORTANT]
>
> The **Public Network Interface** IP address and **Private Hostname** must be hard-coded. Warewulf currently does not support such information to be added to its database. Adapt to your needs if necessary.

```bash
# This block is autogenerated by warewulf
# Host:   {{.BuildHost}}
# Time:   {{.BuildTime}}
# Source: {{.BuildSource}}

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

# External Interface (manually set Public Network Interface IP address)
192.168.2.123 {{$.BuildHost}}
# Internal Interface (manually set Private Hostname)
{{$.Ipaddr}} vmCluster.local

{{- range $node := $.AllNodes}}{{/* for each node */}}
# Entry for {{$node.Id.Get}}
{{- range $devname, $netdev := $node.NetDevs}}{{/* for each network device on the node */}}
{{- if $netdev.Ipaddr.Defined}}{{/* if we have an ip address on this network device */}}
{{- /* emit the node name as hostname if this is the primary */}}
{{$netdev.Ipaddr.Get}} {{if $netdev.Primary.GetB}}{{$node.Id.Get}}.local {{$node.Id.Get}}{{end}} {{$node.Id.Get}}-{{$devname}} {{if $netdev.Device.Defined}}{{$node.Id.Get}}-{{$netdev.Device.Get}}{{end}}
{{- end }}{{/* if ip */}}
{{- end }}{{/* for each network device */}}
{{- end }}{{/* for each node */}}
```

Edit `/srv/warewulf/overlays/generic/rootfs/etc/group.ww` to contain only:

```bash
{{Include "/etc/group"}}
```

Edit `/srv/warewulf/overlays/generic/rootfs/etc/passwd.ww` to contain only:

```bash
# Uncomment the following line to enable passwordless root login
# root::0:0:root:/root:/bin/bas	
{{Include "/etc/passwd"}}
```

Create `/srv/warewulf/overlays/generic/rootfs/etc/subuid.ww` with this content:

```bash
{{Include "/etc/subuid"}}
```

Cretate `/srv/warewulf/overlays/generic/rootfs/etc/subgid.ww` with this content:

```bash
{{Include "/etc/subgid"}}
```

Create `/srv/warewulf/overlays/generic/rootfs/etc/locale.conf.ww` with this content:

```bash
# This block is autogenerated by warewulf
# Host:   {{.BuildHost}}
# Time:   {{.BuildTime}}
# Source: {{.BuildSource}}

{{Include "/etc/locale.conf"}}
```

Create `/srv/warewulf/overlays/generic/rootfs/etc/chrony.conf.ww` with this content:

```bash
# This block is autogenerated by warewulf
# Host:   {{.BuildHost}}
# Time:   {{.BuildTime}}
# Source: {{.BuildSource}}

server {{$.Ipaddr}} minpoll 3 maxpoll 5 iburst
makestep 0.5 -1
```

Create new directory in overlay `generic`:

```bash
wwctl overlay mkdir generic /etc/sysconfig
```

Create `/srv/warewulf/overlays/generic/rootfs/etc/sysconfig/slurmd.ww` with this content:

```bash
# This block is autogenerated by warewulf
# Host:   {{.BuildHost}}
# Time:   {{.BuildTime}}
# Source: {{.BuildSource}}

SLURMD_OPTIONS=--conf-server {{.BuildHost}}
```

Import `MUNGE` key files into overlay `generic`:

```bash
wwctl overlay mkdir generic --mode 0700 /etc/munge
wwctl overlay import generic /etc/munge/munge.key
wwctl overlay chown generic /etc/munge/munge.key $(id -u munge) $(id -g munge)
wwctl overlay chown generic /etc/munge $(id -u munge) $(id -g munge)
```

In case the `MUNGE` key on the frontend-node was regenerated, this step must be repeated. Otherwise the local instance of `slurmd` will not run correctly after deployment of compute-nodes.

### Overlay `host`

Replace `/srv/warewulf/overlays/host/rootfs/etc/hosts.ww` with the following content.

> [!IMPORTANT]
>
> The **Public Network Interface** IP address and **Private Hostname** must be hard-coded. Warewulf currently does not support such information to be added to its database. Adapt to your needs if necessary.

```bash
{{ IncludeBlock "/etc/hosts" "# Do not edit after this line" }}
# This block is autogenerated by warewulf
# Host:   {{.BuildHost}}
# Time:   {{.BuildTime}}
# Source: {{.BuildSource}}

# External Interface (manually set Public Network Interface IP address)
192.168.2.123 {{$.BuildHost}}
# Internal Interface (manually set Private Hostname)
{{$.Ipaddr}} vmCluster.local

{{- range $node := $.AllNodes}}{{/* for each node */}}
# Entry for {{$node.Id.Get}}
{{- range $devname, $netdev := $node.NetDevs}}{{/* for each network device on the node */}}
{{- if $netdev.Ipaddr.Defined}}{{/* if we have an ip address on this network device */}}
{{- /* emit the node name as hostname if this is the primary */}}
{{$netdev.Ipaddr.Get}} {{if $netdev.Primary.GetB}}{{$node.Id.Get}}.local {{$node.Id.Get}}{{end}} {{$node.Id.Get}}-{{$devname}} {{if $netdev.Device.Defined}}{{$node.Id.Get}}-{{$netdev.Device.Get}}{{end}}
{{- end }}{{/* if ip */}}
{{- end }}{{/* for each network device */}}
{{- end }}{{/* for each node */}}
```

Edit `/srv/warewulf/overlays/host/rootfs/etc/dhcpd.conf.ww` and add the following line to the end of the file:

```bash
next-server {{$.Ipaddr}};
```

## Warewulf Configuration

Edit `/etc/warewulf/warewulf.conf` - the main configuration file of the provisioning software.

```bash
WW_INTERNAL: 45
ipaddr: 172.16.79.1					# Private Network Interface
netmask: 255.255.255.0			# Private Netmask
network: 172.16.79.0				# Private Network
...
dhcp:
  enabled: true
  template: static					# use static IP assignment
  range start: 172.16.79.2	# next possible private IP
  range end: 172.16.79.254	# last possible private IP
...
```

> [!NOTE]
>
> **FIXME**: Add /export/apps and /share/apps here later.

Enable and restart `warewulfd` and generate SSH keys for cluster access:

```bash
systemctl enable --now warewulfd
wwctl overlay build
wwctl configure --all
bash /etc/profile.d/ssh_setup.sh
```

Reboot frontend-node and check services:

```bash
systemctl status dhcpd.service
systemctl status tftp.socket
systemctl status nfs-server.service
```

## Define Compute-Node Image for Provisioning

In this section a common compute-node image called `rocky` will be created. Import the base image from Github Container Registry:

```bash
# Container name here: rocky (arbitrarily chosen)
wwctl container import docker://ghcr.io/warewulf/warewulf-rockylinux:9 rocky --syncuser
```

Use option `--force` to overwrite an existing container. Open interactive session on the downloaded container `rocky` and run the given commands inside the container:

```bash
wwctl container shell rocky

# dnf repository inits and package installs
dnf -y install http://repos.openhpc.community/OpenHPC/3/EL_9/x86_64/ohpc-release-3-1.el9.x86_64.rpm
crb enable
dnf -y install file vim xfsprogs kernel kernel-tools nhc-ohpc lmod-ohpc ohpc-base-compute ignition chrony munge ohpc-slurm-client
dnf -y update

# Fixes in Slurm directories and permissions
# https://lists.openhpc.community/g/users/topic/slurm_controller_issue_on/27798593
mkdir /var/log/slurm
mkdir /var/run/slurm
mkdir /var/spool/slurmd
chown slurm:slurm /var/log/slurm
chown slurm:slurm /var/run/slurm

# Required systemd-services on compute-nodes
systemctl enable munge
systemctl enable slurmd
systemctl enable chronyd
systemctl enable chrony-wait	# important!

# set timezone, change according to your location
ln -sf /usr/share/zoneinfo/Europe/Berlin /etc/localtime

exit
```

After `exit` the new image for VNFS container `rocky` is built.

## Slurm Workload Manager

Slurm workload manager requires `mail` or `mailx`, which are [no longer available](https://www.claudiokuenzler.com/blog/1360/where-is-mailx-command-rocky-linux-el-9-s-nail-package). Install and use the replacement:

```bash
dnf -y install postfix s-nail
systemctl enable --now postfix
```

Install the Slurm server meta package and stop master daemon for the moment:

```bash
dnf -y install ohpc-slurm-server
systemctl stop slurmctld
```

Copy OpenHPC-provided default cgroup-config file:

```bash
cp /etc/slurm/cgroup.conf.example /etc/slurm/cgroup.conf
```

Fix some Slurm inconsistencies (https://lists.openhpc.community/g/users/topic/slurm_controller_issue_on/27798593):

```bash
mkdir /var/log/slurm
chown slurm:slurm /var/log/slurm
mkdir /var/run/slurm
chown slurm:slurm /var/run/slurm
```

Create a new subdirectory in overlay `host`:

```bash
wwctl overlay mkdir host /etc/slurm
```

Create file `/srv/warewulf/overlays/host/rootfs/etc/slurm/slurm.conf.ww` with the following content, which will act as Warewulf-controlled template for `/etc/slurm/slurm.conf` on the frontend-node:

```bash
#
# OpenHPC-Rocks
# slurm.conf file to mimic a Rockscluster installation
#
{{- nobackup }}
# This block is autogenerated by warewulf
# Host:   {{.BuildHost}}
# Time:   {{.BuildTime}}
# Source: {{.BuildSource}}
# Help:   https://slurm.schedmd.com/slurm.conf.html
#
# GENERAL
ClusterName={{.Hostname}}
SlurmctldHost={{.BuildHost}}
#SlurmctldHost=
#DisableRootJobs=NO
#EnforcePartLimits=NO
#Epilog=
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=67043328
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=lua
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
MailProg=/usr/bin/mail
#MaxJobCount=10000
#MaxStepCount=40000
#MaxTasksPerNode=512
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/cgroup
#Prolog=
#PrologFlags=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
RebootProgram=/usr/sbin/reboot
ReturnToService=2
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
#TaskEpilog=
#TaskPlugin=task/affinity
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
HealthCheckInterval=30
HealthCheckProgram=/usr/sbin/nhc
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
# SCHEDULING
#DefMemPerCPU=0
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_tres
# This is added to silence the following warning:
# slurmctld: select/cons_tres: select_p_node_init: select/cons_tres SelectTypeParameters not specified, using default value: CR_Core_Memory
SelectTypeParameters=CR_Core_Memory
#
# JOB PRIORITY
#PriorityFlags=
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
#PriorityWeightPartition=
#PriorityWeightQOS=
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
#AccountingStorageHost=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/none
#AccountingStorageUser=
#AccountingStoreFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
#JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#DebugFlags=
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
# OpenHPC default configuration
# Enable the task/affinity plugin to add the --cpu-bind option to srun for GEOPM
TaskPlugin=task/affinity
PropagateResourceLimitsExcept=MEMLOCK
JobCompType=jobcomp/filetxt
Epilog=/etc/slurm/slurm.epilog.clean
#
# COMPUTE NODES
{{- range $node := $.AllNodes}}
{{- $Sockets := 0 }}
{{- $CoresPerSocket := 0}}
{{- $ThreadsPerCore := 0}}
{{- range $tk, $tv := $node.Tags }}
{{- if eq $tk "Sockets" }}
{{- $Sockets = $tv.Get}}
{{- end }}
{{- if eq $tk "CoresPerSocket" }}
{{- $CoresPerSocket = $tv.Get}}
{{- end }}
{{- if eq $tk "ThreadsPerCore" }}
{{- $ThreadsPerCore = $tv.Get}}
{{- end }}
{{- end }}
NodeName={{$node.Id.Get}} Sockets={{$Sockets}} CoresPerSocket={{$CoresPerSocket}} ThreadsPerCore={{$ThreadsPerCore}} State=UNKNOWN
{{- end }}
#
# PARTITION (a.k.a. the batch queue)
PartitionName=main Nodes=ALL Default=YES MaxTime=UNLIMITED State=UP Oversubscribe=EXCLUSIVE
#
# MISC PARAMETERS
# Enable configless option
SlurmctldParameters=enable_configless
# Setup interactive jobs for salloc
LaunchParameters=use_interactive_step
```

> [!IMPORTANT]
>
> Unlike the original OpenHPC description this template-based way of creating `/etc/slurm/slurm.conf` is specificly made for OpenHPC-Rocks.

Have the Slurm central management daemon configuration file created by Warewulf for the first time:

```bash
wwctl overlay build -H
```

Test Slurm central management daemon interactively for other errors:

```bash
/usr/sbin/slurmctld -Dvv
```

If OK, then `CTRL-C` and start and enable daemon with:

```bash
systemctl enable --now slurmctld.service 
```

Test after reboot. The queue `main` is present, but compute-nodes are not yet available.

```bash
[root@vmCluster ~]# sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
main*        up   infinite      0    n/a 
[root@vmCluster ~]#
```

## Update and Enable Firewall

Reenable firewall with:

```bash
systemctl enable --now firewalld.service
```

> [!IMPORTANT]
>
> The following commands assume interface `ens160` as the public network interface and `ens192` as the private network interface. Adjust to your situation if needed.

Both network interfaces on the frontend-node are allocated by default to the `public` firewall zone. This is not the correct choice for the second interface `ens192`. It shall be transferred to the `trusted` zone:

```bash
firewall-cmd --get-active-zones
firewall-cmd --zone=trusted --change-interface=ens192 --permanent
firewall-cmd --get-active-zones
```

Enable services in firewall:

```bash
firewall-cmd --permanent --add-service=warewulf
firewall-cmd --permanent --add-service=dhcp
firewall-cmd --permanent --add-service=nfs
firewall-cmd --permanent --add-service=tftp
firewall-cmd --permanent --add-service=ntp
firewall-cmd --permanent --add-service=dns

# https://fricke.co.uk/Teaching/CS491_2024/Lectures/Lecture_16-Slurm_Installation_and_Config.pdf
firewall-cmd --permanent --zone=trusted --add-port=6817/tcp
firewall-cmd --permanent --zone=trusted --add-port=6819/tcp
# for interactive jobs - use Private Network / Private Netmask
firewall-cmd --permanent --zone=trusted --add-source=172.16.97.0/24
firewall-cmd --reload
```

Continue with the next recipe [Virtualized Compute-Nodes](./04-Virtualized-Compute-Nodes.md).
